---
layout: post
title: SVD 차원축소
subtitle: Feature Selection by Singular Value Decomposition for Reinforcement Learning 사전학습
gh-repo: daattali/beautiful-jekyll
gh-badge: [star, fork, follow]
tags: [SVD]
---



 이번 포스팅에서는 **차원축소(dimension reduction)** 기법으로 널리 쓰이고 있는 **특이값 분해(singular value decomposition)**을 알아보도록 하겠습니다.
 이번 글은 이어질 포스트 **Feature Selection by Singular Value Decomposition for Reinforcement Learning**(Bahram Bechzadiam, 2018) 논문을 이해하기 위한 기본 지식으로 작성하게 되었으며 [ratsgo의 블로그](https://), [bskyvision](https://)의 블로그를 참고했음을 미리 밝힙니다.




내용은 다음과 같이 구성했으며 이미 알고 있는 부분은 가볍게 넘겨도 괜찮습니다.

- 고유값 분해
- 특이값 분해
- 특이값 분해 변형

---
## 고유값 분해 (eigen decomposition)

  우선 고유값 분해에 대해 알아보겠습니다. 고유값 분해는 정방행렬을 대각화하는 방법입니다. 고유값 분해를 통해서 정방행렬은 고유벡터행렬과 고유값행렬로 분해됩니다.

 $ A=S\wedge { S }^{ -1 } $
#####(고유값 분해)

 S는 고유벡터행렬, $\wedge$는 고유값 행렬로 행렬 A의 고유값들을 대각요소로 갖고 있습니다. 만약 행렬 A가 **대칭행렬(symmetric matrix)이라면 공식 1은 아래와 같이 다시 쓰일 수 있습니다.

$ A=Q\wedge { Q }^{ T } $
#####(대칭 행렬의 고유값 분해)

Q 역시 고유벡터행렬이고 고유벡터들이 정규직교(orthonormal)인 직교행렬(orthogonal matrix)이기 때문에 S대신 Q로 표현합니다. 그리고 직교행렬의 역행렬은 그 자신의 전치행렬<sup>transpose</sup>이므로  $  { S }^{ -1 } $ 대신에  $  { Q }^{ T } $로 표현됩니다. ($  { Q }^{ T } $ =$  { Q }^{ -1 } $)


---


## 특이값 분해(Singular value decompostion,SVD)

특이값 분해 역시 고유값 분해와 같이 행렬을 대각화하는 방법입니다. 고유값 분해는 정방행렬에만 사용이 가능했지만, 특이값 분해는 직사각형 행렬일때도 사용할 수 있습니다. 그러다보니 고유값 분해보다 더 일반화되었다고 볼 수 있습니다. $m\times n$ 행렬의 특이값 분해 공식은 아래와 같습니다.
![bandicam 2018-11-25 21-11-39-754 (2) (2)](https://github.com/4someday/4someday.github.io/blob/master/img/bandicam%202018-11-25%2021-11-39-754%20(2)%20(2).gif?raw=true)

$A=U\Sigma { V }^{ T }$
##### (특이값 분해)

여기서 U(m\times m), V(n\times n)는 각 열벡터가 특이벡터(singular vector)인 **특이벡터행렬**이고, $\Sigma$는 r(행렬 A의 rank)특이값(${\sigma}_{1},{\sigma}_{2},...,{\sigma}_{r})$들을 대각요소로 갖고 있는 대각행렬로서 **특이값 행렬**이라고 불립니다.
그리고 고유값 행렬과 다르게 특이값 행렬은 직사각형 행렬 ($m\times n$)입니다.

![daum_equation_1543149597280](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149597280.png?raw=true)
  (m>n)

![daum_equation_1543149573765](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149573765.png?raw=true)
  (m<n)

이젠 각 행렬에 대해서 더 자세히 알아보도록 하겠습니다.

행렬 U와 V에 속한 열벡터(특이벡터)는 서로 직교하는 성질을 지닙니다. 또한 U는   $A{ A }^{ T }$의 V는  ${ A }^{ T }A$의 고유벡터행렬이 됩니다.
![daum_equation_1543151551507](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543151551507.png?raw=true)

$\Sigma$는 대각 성분이 행렬 A의 특이값이고 나머지 성분이 0인 행렬입니다. 따라서, ${ A }^{ T }A$ 또는 $A{ A }^{ T }$에서 0이 아닌 고유값들에 $\sqrt{}$를 씌운 것과 같습니다. (${ A }^{ T }A$ 과 $A{ A }^{ T }$은 동일한 고유값을 가집니다)

  ${ \Sigma  }_{ k }=\sqrt { { \lambda  }_{ k } }$





#### 예제)


----


## 특이값 분해 변형

![bandicam 2018-11-25 22-14-12-777 (2)](https://github.com/4someday/4someday.github.io/blob/master/img/bandicam%202018-11-25%2022-14-12-777%20(2).gif?raw=true)

**thin SVD**
![](http://i.imgur.com/NU5w7Uy.png)

 **thin SVD**는 $Σ$ 행렬의 아랫부분(비대각 파트, 모두 0)과 $U$에서 여기에 해당하는 부분을 모두 제거합니다. 이렇게 U와 Σ를 줄여도 ${U}_{s}Σ_{s}{V}^{T}$로 $A$를 원복할 수 있습니다.

**compact SVD**
![](http://i.imgur.com/2AXD5Fw.png)
 **compact SVD** $Σ$ 행렬에서 비대각파트뿐 아니라 대각원소(특이값)가 0인 부분도 모두 제거한 형태입니다. 여기에 대응하는 $U$와 $V$의 요소 또한 제거합니다. 다시 말해 특이값이 양수인 부분만 골라낸다는 뜻입니다. 이렇게 $U$와 $Σ$, $V$를 줄여도 $U_rΣ_rV^T_r$로 $A$를 원복할 수 있습니다.

**truncated SVD**
![](http://i.imgur.com/CHLt0DM.png)
 **truncated SVD**는 $Σ$ 행렬의 대각원소(특이값) 가운데 상위 t개만 골라낸 형태입니다. 이렇게 하면 행렬 $A$를 원복할 수 없게 되지만, 데이터 정보를 상당히 압축했음에도 행렬 $A$를 근사할 수 있게 됩니다. 이후 설명드릴 잠재의미분석은 바로 이 방법을 사용합니다.
