---
layout: post
title: SVD 차원축소
subtitle: Feature Selection by Singular Value Decomposition for Reinforcement Learning 사전학습
gh-repo: 4someday/4someday.github.io
gh-badge: [star, fork, follow]
tags: [SVD]
---



 이번 포스팅에서는 **차원축소(dimension reduction)** 기법으로 널리 쓰이고 있는 **특이값 분해(singular value decomposition)**을 알아보도록 하겠습니다.
 이번 글은 이어질 포스트 **Feature Selection by Singular Value Decomposition for Reinforcement Learning**(Bahram Bechzadiam, 2018) 논문을 이해하기 위한 기본 지식으로 작성하게 되었으며 [ratsgo의 블로그](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/), [bskyvision의 블로그](http://bskyvision.com/251?category=619292)를 참고했음을 미리 밝힙니다.


내용은 다음과 같이 구성했으며 이미 알고 있는 부분은 가볍게 넘겨도 괜찮습니다.

- 고유값 분해
- 특이값 분해
- 특이값 분해 변형

---

## 고유값 분해 (eigen decomposition)

  우선 고유값 분해에 대해 알아보겠습니다. 고유값 분해는 정방행렬을 대각화하는 방법입니다. 고유값 분해를 통해서 정방행렬은 고유벡터행렬과 고유값행렬로 분해됩니다.

 $ A=S\wedge { S }^{ -1 } $

(고유값 분해)

 S는 고유벡터행렬, $\wedge$는 고유값 행렬로 행렬 A의 고유값들을 대각요소로 갖고 있습니다. 만약 행렬 A가 **대칭행렬(symmetric matrix)**이라면 공식 1은 아래와 같이 다시 쓰일 수 있습니다.

$ A=Q\wedge { Q }^{ T } $

(대칭 행렬의 고유값 분해)

Q 역시 고유벡터행렬이고 고유벡터들이 정규직교(orthonormal)인 직교행렬(orthogonal matrix)이기 때문에 S대신 Q로 표현합니다. 그리고 직교행렬의 역행렬은 그 자신의 전치행렬<sup>transpose</sup>이므로  $  { S }^{ -1 } $ 대신에  $  { Q }^{ T } $로 표현됩니다. ($  { Q }^{ T } $ =$  { Q }^{ -1 } $)


---


## 특이값 분해(Singular value decompostion,SVD)

특이값 분해 역시 고유값 분해와 같이 행렬을 대각화하는 방법입니다. 고유값 분해는 정방행렬에만 사용이 가능했지만, 특이값 분해는 직사각형 행렬일때도 사용할 수 있습니다. 그러다보니 고유값 분해보다 더 일반화되었다고 볼 수 있습니다. $m\times n$ 행렬의 특이값 분해 공식은 아래와 같습니다.


![bandicam 2018-11-25 21-11-39-754 (2) (2)](https://github.com/4someday/4someday.github.io/blob/master/img/bandicam%202018-11-25%2021-11-39-754%20(2)%20(2).gif?raw=true)

$A=U\Sigma { V }^{ T }$

(특이값 분해)

여기서 $U(m\times m)$, $V(n\times n)$는 각 열벡터가 특이벡터<sup>singular vector</sup>인 **특이벡터행렬**이고, $\Sigma$는 r(행렬 A의 rank)특이값( ${\sigma}_{1},{\sigma}_{2},...,{\sigma}_{r}$ )들을 대각요소로 갖고 있는 대각행렬로서 **특이값 행렬**이라고 불립니다.
그리고 고유값 행렬과 다르게 특이값 행렬은 직사각형 행렬 ($m\times n$)입니다.

![daum_equation_1543149597280](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149597280.png?raw=true)
  (m>n)

![daum_equation_1543149573765](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149573765.png?raw=true)
  (m<n)

이젠 각 행렬에 대해서 더 자세히 알아보도록 하겠습니다.

행렬 U와 V에 속한 열벡터(특이벡터)는 서로 직교하는 성질을 지닙니다. 또한 U는   $A{ A }^{ T }$의 V는  ${ A }^{ T }A$의 고유벡터행렬이 됩니다.

![daum_equation_1543151551507](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543151551507.png?raw=true)







#### 예제)


----

