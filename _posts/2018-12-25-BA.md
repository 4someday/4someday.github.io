---
layout: post
title: BA
subtitle: Graph based SSL
gh-repo: 4someday/4someday.github.io
gh-badge: [star, fork, follow]
tags: [SSL]
---


 본 포스팅에서는 semi-supervised learning 방법 중 하나인 Graph based-SSL을 다룰 예정입니다.
 글의 전체적인 내용은 고려대학교 강필성 교수님의 Business-Analytics 강의를 참고하였음을 밝힙니다. 또한 M Hein의 강의자료를 참고함도 밝힙니다.

---

### 가정

먼저 가정에 대해서 알아봅시다.

**가정 : heavy edge로 연결된 인스턴스들은 같은 라벨을 가질 확률이 높다**

Cluster assumption: points which can be connected via (many) paths
through high-density regions are likely to have the same label.

Manifold assumption: each class lies on a separate manifold.



즉, manifold 상의 높은 밀도 지역을 통과하는 path로 연결된 포인트들은 같은 라벨을 가질 확률이 높다는 것입니다

위 그림과 같이 그래프를 그렸을 때 관계가 있는 데이터이면 갖은 라벨을 갖게 해주는 것입니다.



![bandicam 2018-11-25 21-11-39-754 (2) (2)](https://github.com/4someday/4someday.github.io/blob/master/img/20181225_211619.png?raw=true)



___

### Graph construction

Graph-based SSL은 Graph를 기반으로 계산이 됩니다. 베이스로 하는 그래프는 노드와 엣지로 구성이 됩니다.

노드는 라벨된 데이터와 라벨이 되지 않은 데이터 둘 다 포함하는 것입니다. 

엣지는 feature를 통해 계산된 similarity weight입니다.

- k-nearest-neighbor graph, unweighted (0,1 weights)
- fully connected graph, weight decays with distance)
- epsilon-radius graph

![st](https://github.com/4someday/4someday.github.io/blob/master/img/st.png?raw=true)

___

### Neighborhood Methods

- k-nearest-neighbor graph, unweighted (0,1 weights)

  add edges between an instance and its k-nearest neighbors

![1111](https://github.com/4someday/4someday.github.io/blob/master/img/1111.png?raw=true)

- epsilon-radius graph

  add edges to all instances inside a ball of radius e

  ![1111](https://github.com/4someday/4someday.github.io/blob/master/img/2222.png?raw=true)

___



### min-cut algorithm

컷(cut)이 뭔지부터 설명을 하자면, 그래프를 양분하는 건데요.

시작은 무향 그래프이고 가중치도 없는 그래프에서 해봅시다.


![1](https://github.com/4someday/4someday.github.io/blob/master/img/1.png?raw=true?raw=true)



이런 무향 그래프가 있을 때, 정점 A와 정점 E를 분리시키고 싶습니다.



![2](https://github.com/4someday/4someday.github.io/blob/master/img/2.png?raw=true?raw=true)

그래프를 분리시키는 방법은 이렇습니다.

간선 몇 개를 골라서 끊습니다. 끊었을 때 그래프가 두 개의 컴포넌트로 분리되면 성공입니다.

예를 들어, 간선 (B, D), (C, D), (C, E) 3개를 끊으면 그래프가 {A, B, C}, {D, E} 2개의 컴포넌트로 분리됩니다.

이렇게 그래프를 양분시키는 간선 집합을 끊는 것을 컷이라 합니다. 이때 컷을 간단히 그 간선 집합으로 나타내기도 합니다.


컷에는 비용이 있는데, 이렇게 가중치가 없는 그래프에서는 간선 개수가 비용과 같습니다.

지금은 컷의 비용이 3입니다.



![3](https://github.com/4someday/4someday.github.io/blob/master/img/3.png?raw=true)



그런데 이렇게 간선 (A, C), (A, B) 2개를 끊어도 컷이 성립합니다.

이때의 비용은 2로 아까보다 작습니다. 그리고 2보다 적은 비용의 컷은 없습니다.

이렇게 임의의 두 정점을 정하고, 두 정점을 다른 컴포넌트로 분리시키는 최소 비용의 컷을 최소 컷(minimum cut)이라고 칭합니다.



![4](https://github.com/4someday/4someday.github.io/blob/master/img/4.png?raw=true)

가중치가 있는 그래프라면, 간선을 자르는 비용은 그 간선의 가중치가 됩니다.




![5](https://github.com/4someday/4someday.github.io/blob/master/img/5.png?raw=true)



위 그래프에서는 이렇게 간선 (A, C), (B, C), (B, D)를 끊는 것이 비용 9로 최소 컷입니다.





이를 수식화하면

$Fix\quad y_{ l },\quad find\quad { y }_{ u }\in \{ 0,1{ \}  }^{ n-1 }\quad to\quad minimize\quad \sum _{ i,j }^{  }{ { w }_{ ij }\left| { y }_{ i }-{ y }_{ j } \right|  } $


이 됩니다.

그리고 이를 최적화 문제로 바꾸어 해결하면 

$\min _{ y\in { (0,1) }^{ n } }{ \infty \sum _{ i=1 }^{ l }{ { \left( { y }_{ i }-{ y }_{ j } \right)  }^{ 2 } }  } +\sum _{ i,j }^{  }{ { w }_{ ij }\left( { y }_{ i }-{ y }_{ j } \right)  } $

가 됩니다.

___

Harmonic function

discrete한 라벨을 continuous한 값을 줌으로써 harmonic function $f$는 $f(x_i) = y_i for i=1,...,l$을 만족합니다.

Harmonic function는 mincut algorithm에서 추정 labelpart에 해당하는 조건을 완하하여 label을 추정하는 방법입니다.

즉, 추정하려는 label이항상 0또는 1이어야 한다는 정수 가정을 0에서 1사이의 실수로 가정을 완화하는 방법입니다.

이를 위해 harmonicfunction $f(x_i)=y_i$를 사용합니다.

즉 위의 함수가 이와 같이 변하게 됩니다.

$\min _{ y\in { (0,1) }^{ n } }{ \infty \sum _{ i=1 }^{ l }{ { \left( { y }_{ i }-{ y }_{ j } \right)  }^{ 2 } }  } +\sum _{ i,j }^{  }{ { w }_{ ij }\left( f({ x }_{ i })-f({ x }_{ j }) \right) ^{ 2 } } $



___

The graph Laplacian

$f$를 graph laplacian을 이용한 closed form을 이용하여 계산할 수 있습니다.
- $n\times n$ weight matrix W
- Diagonal degree matrix $D : D_ii = \Sigma W_ij$ 
- Graph Laplacian matrix $\triangle=D-W$ 
즉, Graph Laplacian Matrix를 구해서 ‘추정 label part’를 계산하면,

$\min _{ y\in { (0,1) }^{ n } }{ \infty \sum _{ i=1 }^{ l }{ { \left( { y }_{ i }-{ y }_{ j } \right)  }^{ 2 } }  } +f^{ T }\triangle f$

이 됩니다.

![GbSSL_15](https://github.com/4someday/4someday.github.io/blob/master/img/GbSSL_15.png?raw=true)



___

Laplacian을 이용한 Harmonic solution

harmonic solution은 주어진 라벨을 기반으로 에너지를 최소화합니다.

이번엔 기존 label part’의 ∞ penalty 조건을 완화하는 것입니다.

일부 주어진 label들이 틀린 label이면 그것을 보존하는 것보다는 틀림을 인지하고 수정하는 것이 더 좋은 결과물이 될 수 있다고 보기 때문입니다.

- Partition the Laplacian matrix
- Harmonic solution
- The normalized Laplacian is often used


$\min _{ y\in { (0,1) }^{ n } }{ \sum _{ i=1 }^{ l }{ { \left( { y }_{ i }-{ y }_{ j } \right)  }^{ 2 } }  } +\lambda f^{ T }\triangle f$

다시 말해서, ‘기존 label part’에서 ∞가 사라져 조건을 완화하는 대신, 제대로 된 기존 label이 바뀌는 것은 막기 위해 penalty λ를 적용하게 됩니다.

그리고 이에 대한 solution은 아래와 같습니다.

$f=(I+\lambda \triangle )^{-1}y$



___

### code

```{.python}
 import os
 import numpy as np
 import numpy.linalg as lin
 import pandas as pd
 import matplotlib.pyplot as plt
 from scipy import sparse 
 from scipy.sparse.linalg import inv
 from scipy.spatial import distance


 #Class를 나눠 주는 작업과 각 Class를의 index에 대한 정보 저장하기
 class1_idx = (data['V3'] == '1')
 class2_idx = (data['V3'] == '2')
 labeled_idx = (class1_idx | class2_idx)
 unlabeled_idx = (labeled_idx != True)


 class1_idx


 #String 정보를 숫자로 바꿔 주기
 num_samples = data.shape[0]
 y = np.zeros(shape=(num_samples))
 y[class1_idx] = 1
 y[class2_idx] = 2
 y[unlabeled_idx] = 0 
 data['V3'] = y


 #행렬로 변환
 lenght = len(y)
 Yl = np.full((lenght,1),0)
 Yl[class1_idx] = 1
 Yl[class2_idx] = 2
 labeled_lenght = len(y[labeled_idx])


 # RBF kernal 함수에 사용할 Distance Matrix 만들기
 euc = distance.cdist(data.iloc[:, :2], data.iloc[:, :2], 'sqeuclidean')


 # e_radius 함수 만들기
 def e_radius(euc, epsilon):
     if epsilon <= 0:
         print('Use epsilon >= 0')
         return None
     e_distance = np.where(euc < epsilon, euc, np.inf)
     return e_distance


 # e_radius 함수를 가지고 w_matrix 만들기
 def RBF_Weight(euc, epsilon, gamma):
     euc = e_radius(euc, epsilon)

 W = RBF_Weight(euc, epsilon = 1, gamma = 20)


 # Diagonal Dgree Matrix 만들기
 rowsum = W.sum(axis=1)
 D = sparse.diags(rowsum)

 # Laplacian Matrix 만들기
 L = D - W

# Laplacian Matrix 중에서 필요한 Subset Matrix 추출
Luu = L[labeled_lenght:,labeled_lenght:]
Lul = L[labeled_lenght:,:labeled_lenght]

# Unlabeled data에 Label 부여하기
Fu = -lin.inv(Luu)*Lul*Yl[labeled_idx]

# Mincut이 아니기 때문에 1.5보다 큰것은 2로 1.5보다 작은 것은 1로 가게 하기
Fu_lenght = len(Fu)
for i in range(Fu_lenght):
    if Fu[i,0] >= 1.5:
        Fu[i,0] = 2
    else:
        Fu[i,0] = 1

# Total Data에 새로 부여된 labeled data를 추가해주기
Total_y_lenght = len(y[class1_idx]) + len(y[class2_idx]) + len(y[unlabeled_idx])
Total_y =  np.full((Total_y_lenght,1),0)
Total_y[class1_idx] = 1
Total_y[class2_idx] = 2
Total_y[unlabeled_idx] = Fu 

Testdata['V3'] = Total_y
Testdata

plt.scatter(data['V1'],data['V2'],c=data['V3'])
plt.show()

plt.scatter(Testdata['V1'],Testdata['V2'],c=Testdata['V3'])
plt.show()

I = sparse.eye(L.shape[0])
Lam = 2.0

YL = np.full((len(y),1),0)
YL[class1_idx] = 1
YL[class2_idx] = 2
YL[unlabeled_idx] = 1

L.shape
Fu_ = lin.inv(I + Lam*L)*YL

Fu__lenght = len(Fu_)
for i in range(Fu__lenght):
    if Fu_[i,0] >= 1.4:
        Fu_[i,0] = 2
    else:
        Fu_[i,0] = 1

Total_y_lenght_ = len(y[class1_idx]) + len(y[class2_idx]) + len(y[unlabeled_idx])
Total_y_ =  np.full((Total_y_lenght_,1),0)
Total_y_ = Fu_ 
Testdata['V3'] = Total_y_
Testdata
plt.scatter(Testdata['V1'],Testdata['V2'],c=Testdata['V3'])
plt.show()

```
!!
