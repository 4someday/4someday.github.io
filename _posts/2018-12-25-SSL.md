---
layout: post
title: SVD 차원축소
subtitle: Feature Selection by Singular Value Decomposition for Reinforcement Learning 사전학습
gh-repo: 4someday/4someday.github.io
gh-badge: [star, fork, follow]
tags: [SVD]
---


본 포스팅에서는 semi-supervised learning 방법 중 하나인 Graph based-SSL을 다룰 예정입니다.
 글의 전체적인 내용은 고려대학교 강필성 교수님의 Business-Analytics 강의를 참고하였음을 밝힙니다. 또한 M Hein의 강의자료를 참고함도 밝힙니다.

---

## 가정

먼저 가정에 대해서 알아봅시다.

**가정 : heavy edge로 연결된 인스턴스들은 같은 라벨을 가질 확률이 높다**

Cluster assumption: points which can be connected via (many) paths
through high-density regions are likely to have the same label.



Manifold assumption: each class lies on a separate manifold.




즉, manifold 상의 높은 밀도 지역을 통과하는 path로 연결된 포인트들은 같은 라벨을 가질 확률이 높다는 것입니다

![gaph](https://github.com/4someday/4someday.github.io/blob/master/img/404-southpark.jpg)

위 그림과 같이 그래프를 그렸을 때 관계가 있는 데이터이면 갖은 라벨을 갖게 해주는 것입니다.

---

 이번 포스팅에서는 **차원축소(dimension reduction)** 기법으로 널리 쓰이고 있는 **특이값 분해(singular value decomposition)**을 알아보도록 하겠습니다.
 이번 글은 이어질 포스트 **Feature Selection by Singular Value Decomposition for Reinforcement Learning**(Bahram Bechzadiam, 2018) 논문을 이해하기 위한 기본 지식으로 작성하게 되었으며 [ratsgo의 블로그](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/), [bskyvision의 블로그](http://bskyvision.com/251?category=619292)를 참고했음을 미리 밝힙니다.


내용은 다음과 같이 구성했으며 이미 알고 있는 부분은 가볍게 넘겨도 괜찮습니다.

- 고유값 분해
- 특이값 분해
- 특이값 분해 변형

---

## 고유값 분해 (eigen decomposition)

  우선 고유값 분해에 대해 알아보겠습니다. 고유값 분해는 정방행렬을 대각화하는 방법입니다. 고유값 분해를 통해서 정방행렬은 고유벡터행렬과 고유값행렬로 분해됩니다.

 $ A=S\wedge { S }^{ -1 } $

(고유값 분해)

 S는 고유벡터행렬, $\wedge$는 고유값 행렬로 행렬 A의 고유값들을 대각요소로 갖고 있습니다. 만약 행렬 A가 **대칭행렬(symmetric matrix)**이라면 공식 1은 아래와 같이 다시 쓰일 수 있습니다.

$ A=Q\wedge { Q }^{ T } $

(대칭 행렬의 고유값 분해)

Q 역시 고유벡터행렬이고 고유벡터들이 정규직교(orthonormal)인 직교행렬(orthogonal matrix)이기 때문에 S대신 Q로 표현합니다. 그리고 직교행렬의 역행렬은 그 자신의 전치행렬<sup>transpose</sup>이므로  $  { S }^{ -1 } $ 대신에  $  { Q }^{ T } $로 표현됩니다. ($  { Q }^{ T } $ =$  { Q }^{ -1 } $)


---

## 특이값 분해(Singular value decompostion,SVD)

특이값 분해 역시 고유값 분해와 같이 행렬을 대각화하는 방법입니다. 고유값 분해는 정방행렬에만 사용이 가능했지만, 특이값 분해는 직사각형 행렬일때도 사용할 수 있습니다. 그러다보니 고유값 분해보다 더 일반화되었다고 볼 수 있습니다. $m\times n$ 행렬의 특이값 분해 공식은 아래와 같습니다.


![bandicam 2018-11-25 21-11-39-754 (2) (2)](https://github.com/4someday/4someday.github.io/blob/master/img/bandicam%202018-11-25%2021-11-39-754%20(2)%20(2).gif?raw=true)

$A=U\Sigma { V }^{ T }$

(특이값 분해)

여기서 $U(m\times m)$, $V(n\times n)$는 각 열벡터가 특이벡터<sup>singular vector</sup>인 **특이벡터행렬**이고, $\Sigma$는 r(행렬 A의 rank)특이값( $\sigma_1,\sigma_2,...,\sigma_r$ )들을 대각요소로 갖고 있는 대각행렬로서 **특이값 행렬**이라고 불립니다.
그리고 고유값 행렬과 다르게 특이값 행렬은 직사각형 행렬 ($m\times n$)입니다.

![daum_equation_1543149597280](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149597280.png?raw=true)
  (m>n)

![daum_equation_1543149573765](https://github.com/4someday/4someday.github.io/blob/master/img/daum_equation_1543149573765.png?raw=true)
  (m<n)
  
---

___

### Graph construction

Graph-based SSL은 Graph를 기반으로 계산이 됩니다. 베이스로 하는 그래프는 노드와 엣지로 구성이 됩니다.

노드는 라벨된 데이터와 라벨이 되지 않은 데이터 둘 다 포함하는 것입니다. (${ X }_{ l }\cup { X }_{ u }$)

엣지는 feature를 통해 계산된 similarity weight입니다.
- k-nearest-neighbor graph, unweighted (0,1 weights)
- fully connected graph, weight decays with distance)
${ w }_{ ij }\quad =\quad exp(\frac { { -\left\| { x }_{ i }-{ x }_{ j } \right\|  }^{ 2 } }{ { \sigma  }^{ 2 } } )$
- epsilon-radius graph


___

